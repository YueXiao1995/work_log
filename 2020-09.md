# Work Log of September

---

## Week 1

---

### 09-01

* 完成：
  * 审核统计新功能开发
    * **简介**：审核统计加入新功能。在每日进行例行统计时，读取过往历史统计数据并绘制图表，以反映数据在时间维度上的变化趋势和变化量，并将图片加入统计邮件一并发送。 
    * 不再使用原有的urllib，转而使用requests访问邮件服务
    * 解决了使用requests发送post请求并上传多个附件的问题
    * 解决了matplotlib无法在图表中显示中文的问题

---

### 09-02

* 完成：
  * 审核统计新功能开发：
    * 解决了审核统计新功能因为python环境问题无法例行发送的问题
  * 项目代码查看：
    * 打包下载新旧两个审核服务的代码到工作机
    * 查看 梳理机器审核线下模型部分代码（30%）
  * 工作机开发环境配置：
    * 下载安装tensorflow，pytorch到工作机

---

### 09-03

* 完成：
  * 学习TensorFlow官网Tutorial中NLP相关的例子
    * [**Basic text classification**](https://www.tensorflow.org/tutorials/keras/text_classification): 50%
  * 工具开发及数据产出
    * 根据同事需求，开发工具清理书单数据中的涉政涉黄内容，并将处理结果保存。

---

### 09-04

* 完成：
  * 工具开发及数据产出：
    * 继续书单清理工作，根据需求产出多分数据
  * 学习TensorFLow官网Tutorial中NLP相关例子
    * [**Basic text classification**](https://www.tensorflow.org/tutorials/keras/text_classification)： 80%
    * [**Word embeddings**](https://www.tensorflow.org/tutorials/text/word_embeddings)：20%
  * 学习Pytorch官网Tutorial中NLP相关例子
    * [**Text Classification With Torchtext**](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html) ：15%

---

### 09-06

* 完成
  * 学习TensorFlow官网Tutorial中NLP相关的例子
    * [**Basic text classification**](https://www.tensorflow.org/tutorials/keras/text_classification): 90%
    * [**Word embeddings**](https://www.tensorflow.org/tutorials/text/word_embeddings): 90%
    * [**Text calassification with an RNN**](https://www.tensorflow.org/tutorials/text/text_classification_rnn): 5%
* 收获：
  * 文本分类模型中 第一层 embedding层的特征和功能
    * input： 二维tensor（batch_size, input_length）， 一个batch的文本，每条文本由一个序号数组表示，每个序号代表词典中的一个词 
    * output：三维tensor (batch_size, input_length, output_dim)，一个batch的文本，每个文本由一个数组表示，数组中的每一项代表文本中的一个词，每一项又由一个词向量数组表示。
    * weights：词典中的每个词及对应的词向量
  * 熟悉tensorflow中的一些常见函数
  * 了解到了[**Embedding Projector**](http://projector.tensorflow.org/), [**colab**](https://colab.research.google.com/notebooks/intro.ipynb), [**tmux**](https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/)

---

## Week 2

---

### 09-07

* 计划：

  * 学习Pytorch官网Tutorial中NLP相关例子
    * [x]  [**Text Classification With Torchtext**](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)

  * 查看新开发审核服务代码，整理：
    * [x] 原始数据来源，位置
    * [x] 训练数据产生过程
    * [ ] 模型建立，训练，及评估
    * [ ] 模型储存及加载
    * [ ] 模型调用
  * 训练测试
    * [ ] 在服务器上尝试小规模训练数据

* 完成：

  * [**Text Classification With Torchtext**](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)： 80%

  * 查看新开发审核服务代码，整理：

    * 数据来源，位置

    * 数据产生过程

* 收获：

  * 了解到几种用于文本分类的算法，包括RNN和CNN。 
    * 概述：[**Text Classification**](https://monkeylearn.com/text-classification/)
    * RNN or CNN： [**Answers on Quora**](https://www.quora.com/Which-is-better-for-text-classification-CNN-or-RNN-Which-areas-of-NLP-do-they-better-suit-to)
    * TextCNN模型论文：[**Convolutional Neural Networks for Sentence Classification**](https://arxiv.org/pdf/1408.5882.pdf) 
  * 熟悉使用markdown安排每日工作，记录工作日志，提高生产力。 从网上找到了 [**Typora**](https://www.typora.io/) 这款软件，正式开始记录。

---

### 09-08

* 计划：
  * 周报补发
    * [x] 补发上周周报
  * 查看新开发审核服务代码，整理：
    * [x] 模型建立，训练，及评估
    * [x] 模型储存及加载
    * [x] 模型调用
  * 训练测试
    * [ ] 在服务器上尝试小规模训练
  * 工作日志维护
    * [x] 使用git维护工作日志
  * 协同工作
      * [ ] 将一些头部及长约作者加入白名单
* 完成：
    * 补发周报
    * 查看新开发审核服务代码，写注释，整理相关文档：
        * 原始数据
        * 产生供模型训练使用的数据集
        * 模型建立，训练，评估
        * 模型测试
        * 模型在审核服务中的使用
* 收获：
  * [**jeiba**](https://github.com/fxsjy/jieba) 中文分词

---

### 09-09

* 计划：
  * 工作日志整理
    * [x] 整理上周工作日志
  * 训练测试
    * [ ] 在服务器上尝试小规模训练
  * 协同工作
    * [x] 将一些头部及长约作者加入白名单
    * [x] 沟通审核逻辑修改方式

* 完成：
  * 整理上周工作日志
  * 将运营提供的白名单用户加入白名单
  * 沟通审核逻辑修改方式
    * 运营想要实现的结果：白名单用户发布QQ，联系电话时不会被判为违规
    * 结论是：我们无法独立实现这个功能，应该还需要其他部门的配合。但决定先实现一下这个功能。
* 收获：
  * 无

---

### 09-10

* 计划：
  * 协同工作：
    * [x] 根据运营需求修改逻辑代码，使白名单用户可以发送qq，微信号
      * [x] 来自uc的白名单用户请求跳过了token_check 中的qq号检测，和 model_check
      * [x] 来自非uc的白名单用户请求跳过了token_check中的qq号检测
  * 旧项目维护：
    * [x] 机器审核主服务审核逻辑总结
    * [ ] UGC审核服务统计数据及逻辑检查
  * 新项目开发：
    * [ ] 广告识别 模型训练
  * 相关研究
    * [ ] 文本分类 领域 通用模型、技术总结
* 完成：
* 修改审核逻辑，使白名单用户发送的qq号，手机号等消息跳过审核
  * 机审主服务审核逻辑总结
* 收获：
  * 无

---



### 09-11

* 计划：
  * 旧项目维护
    * [x] UGC审核服务统计数据及逻辑检查
  * 新项目开发
    * [x] 训练数据增量产出：从日志中导出正（违规）样本
  * 待办行政事项
    * [x] 完成绩效目标设定表格
  * 业务相关研究
    * [ ] 文本分类 领域 通用模型、技术总结
* 完成：
  * 周会
  * 周报
  * UGC审核服务统计数据及逻辑检查
  * 绩效目标设定表格
  * 训练数据增量产出 ：70%
    * 还有一些机审漏召回的消息需要单独导出
* 收获：
  * 如何评价数据增长的质量



---

### 09-13

* 计划
  * 旧项目维护：
    * [x] 重做：机审主服务审核逻辑总结 （关机未保存导致部分文件丢失）
  * 新开发项目：
    * [ ] 训练数据增量产出：从日志中导出机审漏掉的正（违规）样本
  * 业务相关研究：
    * [ ] 文本分类领域通用模型、技术总结
* 完成
  * 机审主服务审核逻辑总结 
  * 训练数据增量产出： 重构部分代码。
* 收获
  * nothing



---

### Summary

* Day 1
  * [**Text Classification With Torchtext**](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)： 80%
  * 查看新开发审核服务代码，整理：
    * 数据来源，位置
    * 数据产生过程

* Day 2
  * 补发周报
  * 查看新开发审核服务代码，写注释，整理相关文档：
    * 原始数据
    * 产生供模型训练使用的数据集
    * 模型建立，训练，评估
    * 模型测试
    * 模型在审核服务中的使用

* Day 3
  * 整理上周工作日志
  * 将运营提供的白名单用户加入白名单
  * 沟通审核逻辑修改方式
    * 运营想要实现的结果：白名单用户发布QQ，联系电话时不会被判为违规
    * 结论是：我们无法独立实现这个功能，应该还需要其他部门的配合。但决定先实现一下这个功能。

* Day 4
  * 修改审核逻辑，使白名单用户发送的qq号，手机号等消息跳过审核
  * 机审主服务审核逻辑总结

* Day 5
  * UGC审核服务统计数据及逻辑检查
  * 绩效目标设定表格
  * 训练数据增量产出 ：
    * 历遍一个时间段，复用之前的统计代码，产出部分数据

* Day 6
  * 机审主服务审核逻辑总结 
  * 训练数据增量产出： 
    * 重构部分代码，提高可维护性
    * 保存机审漏审的违规信息

---

## Week 3

---

### 09-14

* 计划
  * 新开发项目：
    * [x] 增量训练数据产出：根据审核方以及违规类别保存数据
    * [x] 根据是否违规保存数据
    * [ ] 小数据量测试训练
  * 业务相关研究：
    * [ ] 文本分类领域通用模型、技术总结
* 完成
  * 开发新的样本产出工具，实现了设定时间段后自动生成该时间段内所有样本的功能
  * 产出最近一个月的样本，包括：
    * 机审识别违规 （根据违规类别保存）
    * 人审识别违规
    * 所有违规样本
    * 所有正常样本
* 收获
  * 无

---

### 09-15

* 计划
  * 项目维护：
    * [x] 将机审漏审数据加入统计邮件
  * 新开发项目
    * [ ] 开发工具生成数据集
    * [ ] 下载三个月的数据
    * [ ] 开发二分类模型
  * 业务相关研究：
    * [ ] 文本分类领域通用模型、技术总结
  * 其他：
    * [ ] git配置
* 完成
  * 在新开发的样本产出工具中加入统计功能。 在样本产出的同时，可以产出统计结果。
  * 开发一个例行的 统计+样本产出工具，实现每日的增量数据产出 + 审核结果统计 +邮件发送
  * 修改邮件发送部分代码，并将机审漏审等数据加入每日统计邮件
  * 样本获取。产出了八九两个月的样本，但在产出7月样本时遇到了问题
* 收获
  * 无

---

### 09-16

* 计划：
  * 新开发项目：
    * [x] 将新开发的 增量样本产出+审核统计 加入每日的例行任务，实现每日样本（UGC审核主服务正负样本）的例行产出。
    * [ ] 将UGC统计的 不分类审核结果统计 生成图像并加入统计邮件
    * [x] 解决7月样本不能产出的问题，产出有记录以来的全部数据
    * [ ] 修改k12统计。实现k12样本的例行产出
    * [ ] 开发二分类模型
    * [x] 产出一个merge日志
  * 业务相关研究：
    * [ ] 文本分类领域通用模型、技术总结
  * 其他：
    * [ ] git配置
* 完成
  * 将新开发的 增量样本产出+审核统计工具 加入每日的例行任务，实现每日样本（UGC审核主服务正负样本）的例行产出。
  * 解决7月样本不能产出的问题，产出过去一年（2019-09-16 - 2020-09-16）的全部数据
    * 修改了部分代码以兼容过往不同阶段的日志
    * 产出纯文本样本
    * 产出了一个包含所有正负样本的merge日志，作为以后进行开发的基础数据

* 收获
  * 无

---

### 09-17

* 计划
  * 新开发项目：
    * [x] 将UGC统计的 不分类审核结果案统计 生成图像并加入统计邮件。测试后替代原有的例行任务
    * [ ] 改造k12统计项目，实现k12样本的例行产出
    * [ ] 从k12过往审核日志中产出样本
    * [ ] 将k12统计与趋势生成，邮件发送功能代码分离
    * [ ] 新建一个readme文件，详细介绍生成的数据
    * [ ] 开发二分类模型
  * 旧项目维护
    * [x] 修改机审逻辑，使白名单用户跳过所有检验
  * 业务相关研究
    * [ ] 文本分类领域通用模型，技术总结
* 完成
  * 将UGC统计的 不分类审核结果案统计 生成图像并加入统计邮件。测试后替代原有的例行任务
  * 修改机审逻辑，使白名单用户跳过所有检验
* 收获
  * 无

----

### 09-18

* 计划
  * 新项目开发
    * [ ] 理解熟悉BERT
    * [ ] 尝试跑通模型
* 完成
  * 旧项目维护： 修改返回white user时的状态码
  * 新项目开发：产出前年（2018-09-16 至 2019-09-16）的样本
    * 适配2018-10-31号之前的日志
* 收获
  * 无



---

### Summary

* 完成：
  * 开发样本产出工具，从日志中产出过去一整年（2019-09-16 至今）的正负样本并按照统一格式进行保存
  * 在样本产出中加入统计功能，并加入为例行任务，实现每日例行产出正负样本及统计结果
  * 在统计邮件中加入UGC机审漏审（人工识别违规）等数据及plot图像
  * 修改UGC审核逻辑，使得白名单用户可以跳过所有机审环节，发送QQ号，微信号，电话号码等信息。
* 下周计划：
  * 熟悉BERT
  * 模型开发训练

---

## Week 4

---

### 09-21

* 计划
  * 旧项目维护
    * [x] 修复例行统计的bug
  * 新项目开发
    * [x] 熟悉BERT模型
    * [ ] 模型开发训练
  * 相关领域研究
    * [x] 文本分类领域通用模型，技术总结
* 完成
  * 修复例行统计系统bug，补上过去三天的统计结果
    * 在读取日志时，加个一个filter以确保输入正确
  * 文本分类领域深度学习算法发展历程
  * 修改样本生成与统计项目代码，使得一天之内用户新增的消息的最终状态可以被跟踪，避免一个tid产生多条数据
  * 熟悉BERT模型 15%
* 收获
  * 无

---

### 09-22

* 计划
  * 新项目开发
    * [x] 修改统计部分代码，产出新数据
    * [x] 熟悉BERT模型
* 完成
  * 产出2018.10.31以来的全部样本
  * 熟悉BERT模型的训练和部署 40%
* 收获
  * 无

---

### 09-23

* 计划
  * 新项目开发
    * [x] 修改统计邮件代码，使ugc审核数据为两天前的准确数据
    * [ ] 产出2018.3.2 -2018.10.30 的样本
    * [ ] 在33,32机器上单独部署两个端口，改造机审服务并访问新的模型，用于灰度测试
* 完成
  * 修改统计邮件服务，使ugc审核数据为两天前的准确数据。在邮件末尾加入邮件更新记录。
  * 解决了addpost日志中，因为add和recover的顺序有时候会错乱而导致的问题，删除了原有的数据，重新跑了2020-06-08 到 2020-09-21 的样本，之前的需要重新跑一遍
  * 找出样本中所有单个字符的文本并统计其出现的频次，和被识别为违规的比例
* 收获
  * 无

----

### 09-24

* 计划
  * 新项目开发
    * [ ] 查看103机器上的项目
    * [x] 计算单个字符的违规分数并保存结果
    * [x] 在开发环境修改UGC审核主服务代码，在原有的凡是返回 normal text 的逻辑之前请求一次新的模型
    * [ ] 在33,32机器上单独部署两个端口，改造机审服务并访问新的模型，用于灰度测试
    * [ ] 产出2018-10-31 到 2020-06-07 的样本
* 完成
  * 计算单个字符的违规分数并保存结果
  * 在开发环境修改UGC审核主服务代码，增加一个出参一个入参， 在原有的凡是返回 normal text 的逻辑之前请求一次新的BERT二分类模型
* 收获
  * 监控GPU：[**Linux下监视NVIDIA的GPU使用情况**](https://blog.csdn.net/jasonzzj/article/details/52649174)
    * 命令： `watch -n 10 nvidia-smi`

---

### 09-25

* 计划
  * 新项目开发：
    * [x] 在41机器上部署，测试修改后的项目
    * [ ] 在33,32机器上安装bert-base， 并将修改后的项目部署在两个新端口，测试后，发到审核群
    * [ ] 修改审核统计邮件在邮件中加入高风险，低风险消息的审核结果
    * [ ] 产出2018-10-31 到 2020-06-07 的样本
    * [ ] 在101， 102， 103机器上部署测试bert模型训练，bert项目部署服务
* 完成
  * 修改UGC主审核服务，增加一个入参（switch：是否过bert模型），一个出参（cs：作弊分数），当开关打开时，对所有判定为normal text的文本，再访问一次bert模型
  * 在101机器上开发部署bert代理
  * 部署测试项目
* 收获
  * unicode， utf-8编码
  * 名字空间



---

### 09-27

* 计划
  * 项目维护
    * [x] 找到审核统计邮件发送失败的原因，并修复bug
  * 项目开发
    * [x] 修改UGC审核服务，使得服务随机访问bert代理，代理随机访问bert服务
    * [ ] 修改审核统计服务，增加对高风险，低风险相关消息的统计
    * [ ] 产出 2018-10-31 到 2020-06-07 的样本
* 完成
  * 修复统计邮件bug。（原因：审核部门将网易识别为invalid的内容消息全部找回，导致字典中不存在该类别，以后要用dict.get(key, defaut)方法避免这种情况）
  * 修改UGC审核服务，使得被判别为normal text 的文本随机访问 bert代理，之后代理又随机访问bert服务
  * 将修改后的UGC审核服务部署到线上环境，使用新端口
* 收获
  * nohup命令使程序在后台运行

---

### Summary

* 完成
  * 修改样本生成与统计项目代码，使得一天之内用户新增的消息的最终状态可以被跟踪，避免一个tid产生多条数据。 从日志中重新产出样本。
  * 熟悉BERT模型的部署和训练
  * 修改UGC审核服务代码，增加出参和入参，原来被判定为正常的消息，再过一次新的BERT模型服务
    * bert 服务代理
    * 随机访问机制
  * 修改统计邮件，修复bug
* 下周计划
  * 部署项目，并进行测试
  * 修改统计邮件，加入高风险，低风险信息相关统计

---

## Week 5

---

### 09-28

* 计划
  * [x] 解决33机器新服务无法被访问的问题
  * [x] 把被识别为k12 normal的消息再过一次bert模型
  * [ ] 将bert代理改为一对一访问bert服务，不再随机访问
  * [ ] 将bert代理从tornado改为gunicorn
  * [x] 提交nginx， LVS工单
  * [ ] 压测
* 完成
  * 解决33机器新服务无法被访问的问题。发现错误为端口号缺失。
  * 把被识别为k12 normal的消息再过一次bert模型
  * 提交配置nginx的工单
* 收获
  * 无

---

### 09-29

* 计划
  * [x] 熟悉gunicorn
  * [x] 将bert代理从tornado改为gunicorn，并且改为一对一访问bert服务，不再随机访问 
  * [ ] 在103机器上部署一个bert代理
  * [ ] 更新统计部分代码，实现对高风险，及低风险消息的统计，并加入统计邮件
* 完成
  * 熟悉gunicorn
  * 将bert代理从tornado改为gunicorn，一对一访问bert服务，不再随机访问
* 收获
  * 无

---

### 09-30

* 计划
  * [x] 完成gunicorn配置文件
  * [x] 在101，102上部署新的bert代理（gunicorn+flask）
  * [ ] 优化测试用例
  * [ ] 更新统计部分代码，实现对高风险，及低风险消息的统计，并加入统计邮件
* 完成
  * 完成gunicorn配置文件，环境管理
  * 在101, 102, 103上部署新的bert代理
    * 101机器环境问题没有解决，目前简单用`nohup python3 bert_proxy.py` 部署
* 收获
  * conda 环境克隆
    * conda create -n xiaoyue --clone xxx
  * lsof -i:端口号 查询端口占用

---

## Summary of September

本周完成：

* 修复统计邮件bug。
* 修改UGC审核服务，使得被判别为normal text 的文本访问 bert代理，之后代理又访问bert二分类服务
* 开发部署tornado版的bert部署到101， 102机器，每个代理随机访问两台机器上的bert服务
* 将修改后的UGC审核服务部署到33/32机器的8090端口，随机请求bert代理
* 把被识别为k12 normal的消息也再请求bert服务
* 熟悉gunicorn， 将bert代理从tornado改为gunicorn，并一对一访问bert服务，不再随机访问。后在101, 102, 103上部署。

国庆后计划：

* 压测
* 统计高风险，低风险文本的判断准确率